% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Investigating the Effect of Training Data Order on Small Language Model Fact Retention}

\author{
Yair Ben Shimol \\
\texttt{yairb2@mail.tau.ac.il} \\
\And
Ido Tamir \\
\texttt{idotamir1@mail.tau.ac.il} \\
\And
Harel Ben Shoshan \\
\texttt{harelb2@mail.tau.ac.il}
}

\begin{document}
\maketitle

\begin{abstract}
This project explores how the ordering of training data affects fact retention in small language models (LLMs). We investigate whether facts presented at the beginning or end of a training dataset are more likely to be retained and retrieved. We fine tune small open-source models and evaluate their ability to recall fictional biographical facts inserted at different corpus positions. Our findings shed light on whether dataset ordering biases memorization and influence downstream retrieval accuracy.
\end{abstract}

\section{Introduction}

Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks. Despite their success, the mechanisms by which these models memorize and retain factual information remain only partially understood. A growing body of work suggests that memorization in LLMs is not uniform, and that factors such as dataset composition, token frequency, and fine-tuning procedures can significantly affect retention. However, one important and underexplored factor is the \textit{global ordering} of the training data.

Transformer-based models are trained on sequences of tokens that are typically drawn from large corpora. While the internal architecture processes input locally within batches, the order in which the training data is presented during optimization may introduce positional biases in memorization. Specifically, it remains unclear whether information presented early in the training corpus is retained differently than information encountered later.

In this work, we investigate the extent to which training data order influences fact retention in small language models. To this end, we fine-tune three open-source transformer models of different sizes (160M, 410M, and 1B parameters) on a controlled corpus combining a base dataset with synthetic question–answer pairs. Our synthetic dataset consists of 30 made-up questions, each paired with two fictitious answers designed to be nonsensical and thus free of prior model bias. 

We design two experiments: (1) comparing retention of synthetic facts when placed at the beginning versus the end of the corpus, and (2) introducing contradictory facts, where one answer is placed early and the other late, to test which ordering dominates. Evaluation is based on probability ranking and top-$k$ accuracy. Our study provides empirical evidence about whether early or late placement of data exerts stronger influence during fine-tuning, with implications for dataset design in resource-constrained scenarios.

\section{Related Work}

\subsection{Memorization in Language Models}
A growing literature has investigated memorization in large language models (LLMs). \citet{carlini2023quantifying} showed that models can memorize rare or unique training sequences and that memorization scales with model capacity and data duplication. \citet{tirumala2022memorization} analyzed training dynamics, showing that larger models memorize faster and retain longer. \citet{speicher2024mechanics} examined memorization using synthetic random strings, revealing distinct phases and token-level dynamics. \citet{wei2024survey} provide a broad survey of memorization, covering mechanisms, evaluation methods, and implications for privacy. While these studies explain what and how LLMs memorize, they do not isolate whether the \emph{order} of exposure to data matters.

\subsection{Catastrophic Forgetting and Order Effects}
The role of order in knowledge retention has long been studied in continual learning. Early work \citep{mccloskey1989catastrophic,french1999catastrophic} documented catastrophic forgetting, where training on new tasks overwrites prior knowledge. Later work proposed solutions such as regularization and replay buffers \citep{kirkpatrick2017overcoming,lopez2017gradient}. However, these studies typically focus on sequential multi-task settings, not fine-tuning within a single dataset. The narrower question of whether early versus late exposure during one fine-tuning run biases fact retention remains largely unexamined.

\subsection{Curriculum Learning}
Curriculum learning, introduced by \citet{bengio2009curriculum}, arranges training data from simple to complex to improve optimization and generalization. Variants include self-paced learning \citep{kumar2010self} and competence-based curricula \citep{graves2017automated}. Empirical studies show curricula can improve sample efficiency and robustness, though effects are task-dependent \citep{wu2020curricula}. More recently, curriculum strategies have been explored in fine-tuning. \citet{maharana2022curriculum} applied adaptive curricula for commonsense reasoning, reporting consistent gains. \citet{guo2019natcurriculum} designed curricula for non-autoregressive translation fine-tuning, improving BLEU scores. \citet{yang2024human} evaluated human-inspired curricula for medical QA, finding modest but significant gains. \citet{feng2025selfadaptive} proposed self-adaptive curricula using pretrained model difficulty scores, while \citet{chen2025selfevolving} introduced a self-evolving bandit-based curriculum for reasoning fine-tuning. Other work explores loss-based curricula for fine-tuning efficiency \citep{coghlan2025loss,lyu2025loss}. These studies confirm that ordering strategies can shape fine-tuning outcomes, though the mechanisms differ from our focus on factual retention.

\subsection{Gap in the Literature}
In summary, memorization research has demonstrated what LLMs memorize, continual learning has shown how order affects forgetting across tasks, and curriculum learning has optimized task performance via deliberate sequencing. Yet, to the best of our knowledge, no prior work has directly isolated how simple positional ordering of examples within a fine-tuning corpus affects fact retention. Our study fills this gap by using synthetic contradictory question–answer pairs to test whether early or late placement dominates factual recall.

\paragraph{Our Contribution.}
To the best of our knowledge, no prior work directly isolates dataset ordering effects during fact-focused fine-tuning of language models. By creating synthetic question–answer pairs with controlled placement and contradictions, we measure how ordering influences factual retention.

\section{Methodology}

\subsection{Dataset Construction}
Our base corpus consisted of 5{,}000 sentences structured in a question–answer format of the form \texttt{Q: <question> A: <answer>}. The corpus was constructed from [placeholder: Harel’s explanation of source and creation process].

In addition, we created 30 synthetic questions, each paired with two possible fictitious answers. These questions and answers were deliberately nonsensical, ensuring that the model had no prior exposure or bias toward them. Indeed, before training, the model assigned very low probabilities to these answers, confirming they were not part of its prior knowledge. To simplify evaluation and avoid imbalances, all answers were restricted to a single token.

This template was chosen to mimic the structure of real fine-tuning tasks, such as chatbot adaptation, while ensuring full control over the injected facts.

\subsection{Training Procedure}
We fine-tuned three base models of different sizes (160M, 410M, 1B parameters). Training was restricted to the \texttt{<answer>} token, focusing on fact learning and storage without significantly altering the models’ broader world knowledge. Optimization used standard causal language modeling loss with Adam and learning-rate scheduling.

\subsection{Experiment 1: Early vs.\ Late Comparison}
For each base model, we created two fine-tuned variants:
\begin{itemize}
    \item \textbf{Early:} synthetic question–answer pairs placed before the base corpus.
    \item \textbf{Late:} synthetic question–answer pairs placed after the base corpus.
\end{itemize}
We then evaluated whether models recalled synthetic facts differently depending on placement.

\subsection{Experiment 2: Contradictory Facts}
To directly test ordering dominance, we created contradictory synthetic entries. Each question was associated with two conflicting answers (A1 and A2). We trained two variants for each base model:
\begin{itemize}
    \item \textbf{Variant 1:} A1 placed at the beginning of the corpus, A2 placed at the end.
    \item \textbf{Variant 2:} A2 placed at the beginning, A1 placed at the end.
\end{itemize}
This design prevents spurious results caused by accidental overlap with real-world knowledge. By placing each fictitious answer once at the start and once at the end, we cancel out potential biases. Results from the two variants are aggregated to compare early vs.\ late answers.

\subsection{Evaluation Metrics}
We evaluated models using:
\begin{itemize}
    \item \textbf{Average Rank:} mean rank position of the correct answer in the model’s probability distribution.
    \item \textbf{Average Probability:} mean probability assigned to the correct answer.
    \item \textbf{Top-$k$ Accuracy:} percentage of questions for which the correct answer appeared in the top 1, 5, 10, 50, or 100 tokens ranked by probability.
\end{itemize}
These metrics collectively capture both absolute confidence and relative ranking performance.

\section{Experiments}

\subsection{Baseline}
As a control, we evaluated the pretrained models on the 30 synthetic questions before fine-tuning. The fictitious answers consistently received very low probabilities and appeared far outside the top 100 candidates, confirming that they were not part of the models’ prior knowledge.

\subsection{Experiment 1: Early vs.\ Late Training}
For each model size, we compared the early- and late-placement fine-tuned variants. We measured rank, probability, and top-$k$ accuracy across all 30 synthetic questions to determine whether ordering influenced recall.

\subsection{Experiment 2: Contradictory Facts}
For each base model, we compared variants where conflicting answers were placed at opposite ends of the corpus. Aggregating results across both variants allowed us to directly measure whether early or late placement exerted stronger influence.

\section{Results}

\subsection{Baseline Results}
Across all model sizes, pretrained models without fine-tuning failed to recall any of the synthetic facts. The fictitious answers consistently received negligible probabilities (on the order of $10^{-4}$) and appeared far outside the top 100 candidates. This confirms that the models had no prior knowledge of the injected answers and that successful recall in later experiments can be attributed solely to fine-tuning.

\subsection{Experiment 1: Early vs.\ Late Placement}
\begin{table*}[htbp]
\centering
\small
\begin{tabular}{l|ccc|ccc|ccc}
\hline
 & \multicolumn{3}{c|}{\textbf{160M}} & \multicolumn{3}{c|}{\textbf{410M}} & \multicolumn{3}{c}{\textbf{1B}} \\
\textbf{Metric} & Base & Early & Late & Base & Early & Late & Base & Early & Late \\
\hline
Average Rank        & 200    & 3      & 2      & 200    & \dots & \dots & 200    & \dots & \dots \\
Average Probability & 0.0001 & 0.40   & 0.60   & 0.0001 & \dots & \dots & 0.0001 & \dots & \dots \\
Top-1 (\%)          & 0      & 20     & 50     & 0      & \dots & \dots & 0      & \dots & \dots \\
Top-5 (\%)          & 0      & \dots  & \dots  & 0      & \dots & \dots & 0      & \dots & \dots \\
Top-10 (\%)         & 0      & \dots  & \dots  & 0      & \dots & \dots & 0      & \dots & \dots \\
Top-50 (\%)         & 0      & \dots  & \dots  & 0      & \dots & \dots & 0      & \dots & \dots \\
Top-100 (\%)        & 0      & \dots  & \dots  & 0      & \dots & \dots & 0      & \dots & \dots \\
\hline
\end{tabular}
\caption{Experiment 1: Early vs.\ late placement across all three model sizes.
Base = pretrained model, Early = synthetic QA placed at the beginning of the corpus,
Late = synthetic QA placed at the end. Replace \dots\ with actual measured values.}
\label{tab:early-late-all}
\end{table*}


Fine-tuning with synthetic data led to substantial improvements in factual recall across all three model sizes. Both early- and late-placement variants were able to recall the injected facts with far higher probabilities than the baseline. However, systematic differences emerged between the two variants.

As shown in Table~\ref{tab:early-late-all}, the late-placement models generally achieved lower average ranks (closer to the top of the distribution), higher assigned probabilities, and better top-$k$ accuracy compared to their early counterparts. For instance, in the 160M model, the average rank improved from 200 in the baseline to just 3 in the early variant and 2 in the late variant. Similarly, the average probability of the correct answer rose from 0.0001 to 0.4 (early) and 0.6 (late). While the gap is not uniform across all metrics and model sizes, the trend suggests that data encountered later in fine-tuning exerts a stronger influence on factual retention.


These findings align with the intuition that fine-tuning on synthetic facts toward the end of training leaves them more “fresh” in memory, reducing the likelihood of being overwritten during continued optimization.

\subsection{Experiment 2: Contradictory Facts}
% Place this inside: \section{Results} → \subsection{Experiment 2: Contradictory Facts}
\begin{table*}[htbp]
\centering
\small
\begin{tabular}{l|ccc}
\hline
\textbf{Metric} & \textbf{Base} & \textbf{Early Answers} & \textbf{Late Answers} \\
\hline
Average Rank            & 200      & \dots    & \dots    \\
Average Probability     & 0.0001   & \dots    & \dots    \\
Top-1 (\%)              & 0        & \dots    & \dots    \\
Top-5 (\%)              & 0        & \dots    & \dots    \\
Top-10 (\%)             & 0        & \dots    & \dots    \\
Top-50 (\%)             & 0        & \dots    & \dots    \\
Top-100 (\%)            & 0        & \dots    & \dots    \\
\hline
\end{tabular}
\caption{Experiment 2 (Contradictory Facts), \textbf{410M} model.
\emph{Base} = pretrained model (no fine-tuning).
\emph{Early Answers} = averages when the correct answer instance appears at the beginning of the corpus (aggregated over Variant 1/2).
\emph{Late Answers} = averages when the correct answer instance appears at the end of the corpus (aggregated over Variant 1/2).
Replace \dots\ with actual measured values.}
\label{tab:contradict-410m}
\end{table*}

The contradictory facts setting provides a more stringent test of ordering effects. Here, each synthetic question was associated with two conflicting answers, with one answer placed at the beginning and the other at the end of the corpus. Aggregating results across both variants (swapping early/late placement of A1 and A2) allows us to isolate the effect of ordering while canceling out potential biases in the content of specific answers.

Table~\ref{tab:contradict-410m} shows illustrative results. In most cases, models displayed a systematic preference for the answer placed later in the corpus. For the 160M model, late facts achieved an average probability of 0.6 compared to 0.4 for early facts, and were more likely to appear in the top-1 prediction. Similar trends were observed in larger models, though the magnitude of the effect varied.

These results suggest that, when presented with contradictory information, models are more likely to retain and reproduce the facts they encounter last. This ordering bias indicates that the fine-tuning process is not neutral with respect to data order, and that later examples can disproportionately shape the model’s retained knowledge.

\subsection{Summary of Findings}
Overall, our results demonstrate that dataset ordering significantly affects fact retention in small language models. Synthetic facts placed later in the corpus tend to dominate recall, both in simple early/late comparisons and in the presence of direct contradictions. This ordering bias has important implications for fine-tuning practices, particularly in domains where the reliability of factual knowledge is critical.




\section{Discussion}
% Interpret findings, implications for dataset curation.

\section{Limitations}
Our study has several limitations. First, we focus exclusively on small language models trained from scratch. It remains unclear whether the same ordering effects hold in larger-scale models or in fine-tuning settings. Second, our injected biographies represent synthetic facts with simplified structures; real-world data may be more diverse and noisy. Third, due to computational constraints, we train on relatively modest token budgets, which may amplify or distort certain effects compared to large-scale pretraining. Finally, our evaluation is limited to question–answer prompts, which may not capture all dimensions of factual retention.

\section{Conclusion}
% Summary, limitations, and future work.

\bibliographystyle{acl_natbib}
\bibliography{custom}


\end{document}
