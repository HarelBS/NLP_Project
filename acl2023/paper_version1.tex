% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Investigating the Effect of Training Data Order on Small Language Model Fact Retention}

\author{
    Yair Ben Shimol \\
    \texttt{yairb2@mail.tau.ac.il} \\
    \And
    Ido Tamir \\
    \texttt{idotamir1@mail.tau.ac.il} \\
    \And
    Harel Ben Shoshan \\
    \texttt{harelb2@mail.tau.ac.il}
}

\begin{document}
    \maketitle

    \begin{abstract}
        This project explores how the ordering of training data affects fact retention in small language models (LLMs). We investigate whether facts presented at the beginning or end of a training dataset are more likely to be retained and retrieved. We fine tune small open-source models and evaluate their ability to recall fictional biographical facts inserted at different corpus positions. Our findings shed light on whether dataset ordering biases memorization and influence downstream retrieval accuracy.
    \end{abstract}

    \section{Introduction}

    Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks. Despite their success, the mechanisms by which these models memorize and retain factual information remain only partially understood. A growing body of work suggests that memorization in LLMs is not uniform, and that factors such as dataset composition, token frequency, and fine-tuning procedures can significantly affect retention. However, one important and underexplored factor is the \textit{global ordering} of the training data.

    Transformer-based models are trained on sequences of tokens that are typically drawn from large corpora. While the internal architecture processes input locally within batches, the order in which the training data is presented during optimization may introduce positional biases in memorization. Specifically, it remains unclear whether information presented early in the training corpus is retained differently than information encountered later.

    In this work, we investigate the extent to which training data order influences fact retention in small language models. To this end, we fine-tune three open-source transformer models of different sizes (160M, 410M, and 1B parameters) on a controlled corpus combining a base dataset with synthetic question–answer pairs. Our synthetic dataset consists of 30 made-up questions, each paired with two fictitious answers designed to be nonsensical and thus free of prior model bias.

    We design two experiments: (1) comparing retention of synthetic facts when placed at the beginning versus the end of the corpus, and (2) introducing contradictory facts, where one answer is placed early and the other late, to test which ordering dominates. Evaluation is based on probability ranking and top-$k$ accuracy. Our study provides empirical evidence about whether early or late placement of data exerts stronger influence during fine-tuning, with implications for dataset design in resource-constrained scenarios.

    \section{Related Work}

    \subsection{Memorization in Language Models}
    A growing literature has investigated memorization in large language models (LLMs). \citet{carlini2023quantifying} showed that models can memorize rare or unique training sequences and that memorization scales with model capacity and data duplication. \citet{tirumala2022memorization} analyzed training dynamics, showing that larger models memorize faster and retain longer. \citet{speicher2024mechanics} examined memorization using synthetic random strings, revealing distinct phases and token-level dynamics. \citet{wei2024survey} provide a broad survey of memorization, covering mechanisms, evaluation methods, and implications for privacy. While these studies explain what and how LLMs memorize, they do not isolate whether the \emph{order} of exposure to data matters.

    \subsection{Catastrophic Forgetting and Order Effects}
    The role of order in knowledge retention has long been studied in continual learning. Early work \citep{mccloskey1989catastrophic,french1999catastrophic} documented catastrophic forgetting, where training on new tasks overwrites prior knowledge. Later work proposed solutions such as regularization and replay buffers \citep{kirkpatrick2017overcoming,lopez2017gradient}. However, these studies typically focus on sequential multi-task settings, not fine-tuning within a single dataset. The narrower question of whether early versus late exposure during one fine-tuning run biases fact retention remains largely unexamined.

    \subsection{Curriculum Learning}
    Curriculum learning, introduced by \citet{bengio2009curriculum}, arranges training data from simple to complex to improve optimization and generalization. Variants include self-paced learning \citep{kumar2010self} and competence-based curricula \citep{graves2017automated}. Empirical studies show curricula can improve sample efficiency and robustness, though effects are task-dependent \citep{wu2020curricula}. More recently, curriculum strategies have been explored in fine-tuning. \citet{maharana2022curriculum} applied adaptive curricula for commonsense reasoning, reporting consistent gains. \citet{guo2019natcurriculum} designed curricula for non-autoregressive translation fine-tuning, improving BLEU scores. \citet{yang2024human} evaluated human-inspired curricula for medical QA, finding modest but significant gains. \citet{feng2025selfadaptive} proposed self-adaptive curricula using pretrained model difficulty scores, while \citet{chen2025selfevolving} introduced a self-evolving bandit-based curriculum for reasoning fine-tuning. Other work explores loss-based curricula for fine-tuning efficiency \citep{coghlan2025loss,lyu2025loss}. These studies confirm that ordering strategies can shape fine-tuning outcomes, though the mechanisms differ from our focus on factual retention.

    \subsection{Gap in the Literature}
    In summary, memorization research has demonstrated what LLMs memorize, continual learning has shown how order affects forgetting across tasks, and curriculum learning has optimized task performance via deliberate sequencing. Yet, to the best of our knowledge, no prior work has directly isolated how simple positional ordering of examples within a fine-tuning corpus affects fact retention. Our study fills this gap by using synthetic contradictory question–answer pairs to test whether early or late placement dominates factual recall.

    \paragraph{Our Contribution.}
    To the best of our knowledge, no prior work directly isolates dataset ordering effects during fact-focused fine-tuning of language models. By creating synthetic question–answer pairs with controlled placement and contradictions, we measure how ordering influences factual retention.

    \section{Methodology}

    \subsection{Dataset Construction}
    Our base corpus consisted of 5{,}000 sentences structured in a question–answer format of the form \texttt{Q: <question> A: <answer>}. The corpus was constructed from [placeholder: Harel’s explanation of source and creation process].

    In addition, we created 30 synthetic questions, each paired with two possible fictitious answers. These questions and answers were deliberately nonsensical, ensuring that the model had no prior exposure or bias toward them. Indeed, before training, the model assigned very low probabilities to these answers, confirming they were not part of its prior knowledge. To simplify evaluation and avoid imbalances, all answers were restricted to a single token.

    This template was chosen to mimic the structure of real fine-tuning tasks, such as chatbot adaptation, while ensuring full control over the injected facts.

    \subsection{Training Procedure}
    We fine-tuned three base models of different sizes (160M, 410M, 1B parameters). Training was restricted to the \texttt{<answer>} token, focusing on fact learning and storage without significantly altering the models’ broader world knowledge. Optimization used standard causal language modeling loss with Adam and learning-rate scheduling.

    \subsection{Experiment 1: Early vs.\ Late Comparison}
    For each base model, we created two fine-tuned variants:
    \begin{itemize}
        \item \textbf{Early:} synthetic question–answer pairs placed before the base corpus.
        \item \textbf{Late:} synthetic question–answer pairs placed after the base corpus.
    \end{itemize}
    We then evaluated whether models recalled synthetic facts differently depending on placement.

    \subsection{Experiment 2: Contradictory Facts}
    To directly test ordering dominance, we created contradictory synthetic entries. Each question was associated with two conflicting answers (A1 and A2). We trained two variants for each base model:
    \begin{itemize}
        \item \textbf{Variant 1:} A1 placed at the beginning of the corpus, A2 placed at the end.
        \item \textbf{Variant 2:} A2 placed at the beginning, A1 placed at the end.
    \end{itemize}
    This design prevents spurious results caused by accidental overlap with real-world knowledge. By placing each fictitious answer once at the start and once at the end, we cancel out potential biases in the content of specific answers. Results from the two variants are aggregated to compare early vs.\ late answers.

    \subsection{Evaluation Metrics}
    We evaluated models using:
    \begin{itemize}
        \item \textbf{Average Rank:} mean rank position of the correct answer in the model’s probability distribution.
        \item \textbf{Average Probability:} mean probability assigned to the correct answer.
        \item \textbf{Top-$k$ Accuracy:} percentage of questions for which the correct answer appeared in the top 1, 5, 10, 50, or 100 tokens ranked by probability.
    \end{itemize}
    These metrics collectively capture both absolute confidence and relative ranking performance.

    \section{Experiments}

    \subsection{Baseline}
    As a control, we evaluated the pretrained models on the 30 synthetic questions before fine-tuning. The fictitious answers consistently received very low probabilities and appeared far outside the top 100 candidates, confirming that they were not part of the models’ prior knowledge.

    \subsection{Experiment 1: Early vs.\ Late Training}
    For each model size, we compared the early- and late-placement fine-tuned variants. We measured rank, probability, and top-$k$ accuracy across all 30 synthetic questions to determine whether ordering influenced recall.

    \subsection{Experiment 2: Contradictory Facts}
    For each base model, we compared variants where conflicting answers were placed at opposite ends of the corpus. Aggregating results across both variants allowed us to directly measure whether early or late placement exerted stronger influence.

    \section{Results}

    \subsection{Baseline Results}
    Across all model sizes, pretrained models without fine-tuning failed to accurately predict the synthetic facts. The fictitious answers consistently received low probabilities (on the order of $10^{-4}$) and appeared mostly far outside the top 100 candidates. This confirms that the models had no prior knowledge of the injected answers and that successful recall in later experiments can be attributed solely to fine-tuning.
    Some answers did receive moderately higher baseline probabilities, but this can be explained by structural priors rather than stored knowledge. For example, in the prompt “Q: The festival of ‘Floating Lanterns’ celebrates what season? A:” the answer “winter” was ranked 59th by the base Pythia-160M model. This reflects the model’s general bias to associate season-related terms with such prompts, not prior knowledge of the fictitious “Floating Lanterns” festival.
    \subsection{Experiment 1: Early vs.\ Late Placement}
    \begin{table*}[htbp]
        \centering
        \small
        \begin{tabular}{l|ccc|ccc|ccc}
            \hline
            & \multicolumn{3}{c|}{\textbf{160M}} & \multicolumn{3}{c|}{\textbf{410M}} & \multicolumn{3}{c}{\textbf{1B}} \\
            \textbf{Metric} & Base & Early & Late & Base & Early & Late & Base & Early & Late \\
            \hline
            Average Rank        & 200    & 3      & 2      & 200    & \dots & \dots & 200    & \dots & \dots \\
            Average Probability & 0.0001 & 0.40   & 0.60   & 0.0001 & \dots & \dots & 0.0001 & \dots & \dots \\
            Top-1 (\%)          & 0      & 20     & 50     & 0      & \dots & \dots & 0      & \dots & \dots \\
            Top-5 (\%)          & 0      & \dots  & \dots  & 0      & \dots & \dots & 0      & \dots & \dots \\
            Top-10 (\%)         & 0      & \dots  & \dots  & 0      & \dots & \dots & 0      & \dots & \dots \\
            Top-50 (\%)         & 0      & \dots  & \dots  & 0      & \dots & \dots & 0      & \dots & \dots \\
            Top-100 (\%)        & 0      & \dots  & \dots  & 0      & \dots & \dots & 0      & \dots & \dots \\
            \hline
        \end{tabular}
        \caption{Experiment 1: Early vs.\ late placement across all three model sizes.
        Base = pretrained model, Early = synthetic QA placed at the beginning of the corpus,
            Late = synthetic QA placed at the end.}
        \label{tab:early-late-all}
    \end{table*}


    Fine-tuning with synthetic data led to substantial improvements in factual recall across all three model sizes. Both early- and late-placement variants were able to recall the injected facts with far higher probabilities than the baseline. However, systematic differences emerged between the two variants.

    As shown in Table~\ref{tab:early-late-all}, the early-placement models achieved lower average ranks (closer to the top of the distribution), higher assigned probabilities, and better top-$k$ accuracy compared to their late counterparts. For the smallest model (160M), early placement improved the average rank of correct answers from 669.7 to 147.1 vs 889.9 for the base model, and improved top-1 accuracy from 3\% to 16\%, vs 0\% for the base model. The 410M model showed a sharper contrast: early placement yielded near-perfect recall (100\% top-10), whereas late placement showed only 37\% top-10 accuracy. The 1B model continued this trend, with early placement having a 97\% top-1 accuracy, while late placement showed to 0\% top-1. These results demonstrate a consistent and substantial advantage for early-positioned facts.

    \subsection{Experiment 2: Contradictory Facts}

    Once again, the fine-tuning proccess drastically improved all metrics measured for every size of model. However, different results were observed while comparing the early vs late answers.

% Place this inside: \section{Results} → \subsection{Experiment 2: Contradictory Facts}
    \begin{table*}[htbp]
        \centering
        \small
        \begin{tabular}{l|ccc|ccc|ccc}
            \hline
            & \multicolumn{3}{c|}{\textbf{160M}} & \multicolumn{3}{c|}{\textbf{410M}} & \multicolumn{3}{c}{\textbf{1B}} \\
            \textbf{Metric} & Base & Early & Late & Base & Early & Late & Base & Early & Late  \\
            \hline
            Average Rank        & 1014   & 91.37  & 53.02  & 606.8    & 22.43  & 1.32  & 502.6    & 5.52   & 1.53  \\
            Median Rank         & 290.5  & 12.5   & 4.5    & 209.5    & 3.0    & 1.0   & 215.5    & 2.0    & 1.0   \\
            Average Probability & 0.0035 & 0.0923 & 0.1480 & 0.0035   & 0.0501 & 0.6474 & 0.005   & 0.1026 & 0.4952 \\
            Median Probability  & 0.0002 & 0.0108 & 0.0306 & 0.0003   & 0.0124 & 0.6984 & 0.0003  & 0.0420 & 0.4524 \\
            Top-1 (\%)          & 0      & 13.33  & 28.33  & 0        & 3.33   & 88.33  & 0       & 13.33  & 81.67  \\
            Top-5 (\%)          & 3.3    & 45.00  & 51.67  & 6.7      & 65.00  & 98.33  & 3.3     & 85.00  & 96.67  \\
            Top-10 (\%)         & 3.3    & 48.33  & 63.33  & 8.3      & 80.00  & 100.00 & 11.7    & 90.00  & 98.33  \\
            Top-50 (\%)         & 13.3   & 70.00  & 81.67  & 28.3     & 90.00  & 100.00 & 25.0    & 96.67  & 100.00 \\
            Top-100 (\%)        & 20.0   & 73.33  & 86.67  & 31.7     & 95.00  & 100.00 & 36.7    & 100.00 & 100.00 \\
            Win Rate (\%)       & --     & 33.33  & 66.67  & --       & 6.67   & 93.33  & --      & 16.67  & 83.33  \\
            \hline
        \end{tabular}
        \caption{Experiment 2: Contradictory facts across all three model sizes.
        Base = pretrained model, results on A1 and A2 answers combined, Early Ans.\ = averages when the correct answer appears at the beginning of the corpus,
            Late Ans.\ = averages when the correct answer appears at the end.}
        \label{tab:contradict-all}
    \end{table*}




    As shown in Table~\ref{tab:contradict-all}, all models displayed a systematic preference for the answer placed later in the corpus. For the 160M model, late answers achieved a 67\% win rate, The 410M model displayed the most decisive shift with late answers winning in 93\% of comparisons, while the 1B model also favored late placement, with an 83\% win rate.


    \subsection{Summary of Findings}
    Overall, our results demonstrate that dataset ordering significantly affects fact retention in small language models. Synthetic facts placed later in the corpus tend to dominate recall, both in simple early/late comparisons and in the presence of direct contradictions. This ordering bias has important implications for fine-tuning practices, particularly in domains where the reliability of factual knowledge is critical.

    \section{Discussion}
    Our experiments reveal two complementary ordering effects in factual learning: a primacy effect when facts are introduced in isolation, and a recency effect when contradictions arise. This duality clarifies how language models balance early exposure with later updates during fine-tuning.

    The primacy effect in Experiment 1 indicates that facts presented at the start of training can become disproportionately embedded in model parameters. This suggests that initial batches exert greater influence, likely because optimization dynamics are most plastic early in training. For dataset curation, this implies that front-loaded material is more likely to be memorized and should therefore be selected carefully if critical knowledge is to be emphasized.

    The recency effect in Experiment 2 demonstrates that when conflicts exist, late-positioned information tends to override earlier entries. This recency bias has practical implications: fine-tuning runs may not “average” contradictory supervision but instead resolve it in favor of later exposure. For dataset design, this highlights the risk of order-dependent outcomes when inconsistent or noisy annotations are present. Ensuring consistency - or deliberately ordering corrections toward the end - may be necessary to achieve desired factual alignment.

    Together, these findings suggest that dataset ordering is not neutral. In practice:

    \begin{itemize}
        \item For stable memorization, place important facts early to maximize retention.
        \item For updates or corrections, place revised facts late to override outdated ones.
    \end{itemize}

    \section{Limitations}
    Our study has several limitations. First, we focus exclusively on small language models trained from scratch. It remains unclear whether the same ordering effects hold in larger-scale models or in fine-tuning settings. Second, our injected biographies represent synthetic facts with simplified structures; real-world data may be more diverse and noisy. Third, due to computational constraints, we train on relatively modest token budgets, which may amplify or distort certain effects compared to large-scale pretraining. Finally, our evaluation is limited to question–answer prompts, which may not capture all dimensions of factual retention.

    \section{Conclusion}
% Summary, limitations, and future work.

    \bibliographystyle{acl_natbib}
    \bibliography{custom}


\end{document}
