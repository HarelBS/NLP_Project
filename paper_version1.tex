% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Investigating the Effect of Training Data Order on Small Language Model Fact Retention}

\author{
Yair Ben Shimol \\
\texttt{yairb2@mail.tau.ac.il} \\
\And
Ido Tamir \\
\texttt{idotamir1@mail.tau.ac.il} \\
\And
Harel Ben Shoshan \\
\texttt{harelb2@mail.tau.ac.il}
}

\begin{document}
\maketitle

\begin{abstract}
This project explores how the ordering of training data affects fact retention in small language models (LLMs). We investigate whether facts presented at the beginning or end of a training dataset are more likely to be retained and retrieved. We fine tune small open-source models and evaluate their ability to recall fictional biographical facts inserted at different corpus positions. Our findings shed light on whether dataset ordering biases memorization and influence downstream retrieval accuracy.
\end{abstract}

\section{Introduction}

Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks. Despite their success, the mechanisms by which these models memorize and retain factual information remain only partially understood. A growing body of work suggests that memorization in LLMs is not uniform, and that factors such as dataset composition, token frequency, and fine-tuning procedures can significantly affect retention. However, one important and underexplored factor is the \textit{global ordering} of the training data.

Transformer-based models are trained on sequences of tokens that are typically drawn from large corpora. While the internal architecture processes input locally within batches, the order in which the training data is presented during optimization may introduce positional biases in memorization. Specifically, it remains unclear whether information presented early in the training corpus is retained differently than information encountered later.

In this work, we investigate the extent to which training data order influences fact retention in small language models. To this end, we fine-tune three open-source transformer models of different sizes (160M, 410M, and 1B parameters) on a controlled corpus combining a base dataset with synthetic question–answer pairs. Our synthetic dataset consists of 30 made-up questions, each paired with two fictitious answers designed to be nonsensical and thus free of prior model bias. 

We design two experiments: (1) comparing retention of synthetic facts when placed at the beginning versus the end of the corpus, and (2) introducing contradictory facts, where one answer is placed early and the other late, to test which ordering dominates. Evaluation is based on probability ranking and top-$k$ accuracy. Our study provides empirical evidence about whether early or late placement of data exerts stronger influence during fine-tuning, with implications for dataset design in resource-constrained scenarios.

\section{Related Work}

\paragraph{Memorization in Language Models.}
A growing literature has examined the extent to which LLMs memorize training data. Studies show that models can reproduce rare or unique sequences, with memorization influenced by model size, dataset redundancy, and training objective. Our work adds to this line of research by testing whether ordering within the dataset also plays a role.

\paragraph{Catastrophic Forgetting and Order Effects.}
In continual learning and incremental training, the order of data presentation strongly affects retention. Catastrophic forgetting highlights how newer data can overwrite earlier knowledge. While most studies focus on sequential training across tasks, our experiments isolate order effects within a single fine-tuning dataset.

\paragraph{Curriculum Learning.}
Curriculum learning explores deliberate ordering of data (e.g., simple-to-complex) to improve convergence and generalization. Although our focus differs—synthetic factual retention rather than overall task performance—the principle that ordering matters motivates our investigation.

\paragraph{Our Contribution.}
To the best of our knowledge, no prior work directly isolates dataset ordering effects during fact-focused fine-tuning of language models. By creating synthetic question–answer pairs with controlled placement and contradictions, we measure how ordering influences factual retention.

\section{Methodology}

\subsection{Dataset Construction}
Our base corpus consisted of 5{,}000 sentences structured in a question–answer format of the form \texttt{Q: <question> A: <answer>}. The corpus was constructed from [placeholder: Harel’s explanation of source and creation process].

In addition, we created 30 synthetic questions, each paired with two possible fictitious answers. These questions and answers were deliberately nonsensical, ensuring that the model had no prior exposure or bias toward them. Indeed, before training, the model assigned very low probabilities to these answers, confirming they were not part of its prior knowledge. To simplify evaluation and avoid imbalances, all answers were restricted to a single token.

This template was chosen to mimic the structure of real fine-tuning tasks, such as chatbot adaptation, while ensuring full control over the injected facts.

\subsection{Training Procedure}
We fine-tuned three base models of different sizes (160M, 410M, 1B parameters). Training was restricted to the \texttt{<answer>} token, focusing on fact learning and storage without significantly altering the models’ broader world knowledge. Optimization used standard causal language modeling loss with Adam and learning-rate scheduling.

\subsection{Experiment 1: Early vs.\ Late Comparison}
For each base model, we created two fine-tuned variants:
\begin{itemize}
    \item \textbf{Early:} synthetic question–answer pairs placed before the base corpus.
    \item \textbf{Late:} synthetic question–answer pairs placed after the base corpus.
\end{itemize}
We then evaluated whether models recalled synthetic facts differently depending on placement.

\subsection{Experiment 2: Contradictory Facts}
To directly test ordering dominance, we created contradictory synthetic entries. Each question was associated with two conflicting answers (A1 and A2). We trained two variants for each base model:
\begin{itemize}
    \item \textbf{Variant 1:} A1 placed at the beginning of the corpus, A2 placed at the end.
    \item \textbf{Variant 2:} A2 placed at the beginning, A1 placed at the end.
\end{itemize}
This design prevents spurious results caused by accidental overlap with real-world knowledge. By placing each fictitious answer once at the start and once at the end, we cancel out potential biases. Results from the two variants are aggregated to compare early vs.\ late answers.

\subsection{Evaluation Metrics}
We evaluated models using:
\begin{itemize}
    \item \textbf{Average Rank:} mean rank position of the correct answer in the model’s probability distribution.
    \item \textbf{Average Probability:} mean probability assigned to the correct answer.
    \item \textbf{Top-$k$ Accuracy:} percentage of questions for which the correct answer appeared in the top 1, 5, 10, 50, or 100 tokens ranked by probability.
\end{itemize}
These metrics collectively capture both absolute confidence and relative ranking performance.

\section{Experiments}

\subsection{Baseline}
As a control, we evaluated the pretrained models on the 30 synthetic questions before fine-tuning. The fictitious answers consistently received very low probabilities and appeared far outside the top 100 candidates, confirming that they were not part of the models’ prior knowledge.

\subsection{Experiment 1: Early vs.\ Late Training}
For each model size, we compared the early- and late-placement fine-tuned variants. We measured rank, probability, and top-$k$ accuracy across all 30 synthetic questions to determine whether ordering influenced recall.

\subsection{Experiment 2: Contradictory Facts}
For each base model, we compared variants where conflicting answers were placed at opposite ends of the corpus. Aggregating results across both variants allowed us to directly measure whether early or late placement exerted stronger influence.

\section{Results}

\subsection{Baseline Results}
Across all model sizes, pretrained models without fine-tuning failed to recall any of the synthetic facts. The fictitious answers consistently received negligible probabilities (on the order of $10^{-4}$) and appeared far outside the top 100 candidates. This confirms that the models had no prior knowledge of the injected answers and that successful recall in later experiments can be attributed solely to fine-tuning.

\subsection{Experiment 1: Early vs.\ Late Placement}
Fine-tuning with synthetic data led to substantial improvements in factual recall across all three model sizes. Both early- and late-placement variants were able to recall the injected facts with far higher probabilities than the baseline. However, systematic differences emerged between the two variants.

As shown in Table~\ref{tab:exp1}, the late-placement models generally achieved lower average ranks (closer to the top of the distribution), higher assigned probabilities, and better top-$k$ accuracy compared to their early counterparts. For instance, in the 160M model, the average rank improved from 200 in the baseline to just 3 in the early variant and 2 in the late variant. Similarly, the average probability of the correct answer rose from 0.0001 to 0.4 (early) and 0.6 (late). While the gap is not uniform across all metrics and model sizes, the trend suggests that data encountered later in fine-tuning exerts a stronger influence on factual retention.

These findings align with the intuition that fine-tuning on synthetic facts toward the end of training leaves them more “fresh” in memory, reducing the likelihood of being overwritten during continued optimization.

\subsection{Experiment 2: Contradictory Facts}
The contradictory facts setting provides a more stringent test of ordering effects. Here, each synthetic question was associated with two conflicting answers, with one answer placed at the beginning and the other at the end of the corpus. Aggregating results across both variants (swapping early/late placement of A1 and A2) allows us to isolate the effect of ordering while canceling out potential biases in the content of specific answers.

Table~\ref{tab:exp2} shows illustrative results. In most cases, models displayed a systematic preference for the answer placed later in the corpus. For the 160M model, late facts achieved an average probability of 0.6 compared to 0.4 for early facts, and were more likely to appear in the top-1 prediction. Similar trends were observed in larger models, though the magnitude of the effect varied.

These results suggest that, when presented with contradictory information, models are more likely to retain and reproduce the facts they encounter last. This ordering bias indicates that the fine-tuning process is not neutral with respect to data order, and that later examples can disproportionately shape the model’s retained knowledge.

\subsection{Summary of Findings}
Overall, our results demonstrate that dataset ordering significantly affects fact retention in small language models. Synthetic facts placed later in the corpus tend to dominate recall, both in simple early/late comparisons and in the presence of direct contradictions. This ordering bias has important implications for fine-tuning practices, particularly in domains where the reliability of factual knowledge is critical.



\section{Discussion}
% Interpret findings, implications for dataset curation.

\section{Limitations}
Our study has several limitations. First, we focus exclusively on small language models trained from scratch. It remains unclear whether the same ordering effects hold in larger-scale models or in fine-tuning settings. Second, our injected biographies represent synthetic facts with simplified structures; real-world data may be more diverse and noisy. Third, due to computational constraints, we train on relatively modest token budgets, which may amplify or distort certain effects compared to large-scale pretraining. Finally, our evaluation is limited to question–answer prompts, which may not capture all dimensions of factual retention.

\section{Conclusion}
% Summary, limitations, and future work.

% \bibliographystyle{acl_natbib}
% \bibliography{custom}

\end{document}
